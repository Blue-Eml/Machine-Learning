---
title: "Chap 7 Homework"
author: "Emily Blue"
date: "`3.1.24`"
output: openintro::lab_report
---

```{r load-packages, message=FALSE}
library(tidyverse)
library(openintro)
library(dplyr)
library(ISLR2)
library(boot)
library(splines)
library(gam)
library(leaps)
```

## From Chapter 7, Section 7.9:

# Exercise 6

Perform polynomial regression to predict wage using age. Use cross-validation to select the optimal degree d for the polynomial

a)
```{r code-chunk-label}

# 4th degree polynomial
fit <- lm(wage ~ poly(age, 4), data = Wage)
coef(summary(fit))


## Hypothesis Testing Using ANOVA

fit.1 <- lm(wage ~ age, data = Wage)
fit.2 <- lm(wage ~ poly(age, 2), data = Wage)
fit.3 <- lm(wage ~ poly(age, 3), data = Wage)
fit.4 <- lm(wage ~ poly(age, 4), data = Wage)
fit.5 <- lm(wage ~ poly(age, 5), data = Wage)

options(scipen = 999)
anova(fit.1, fit.2, fit.3, fit.4, fit.5)
# The anova model indicates either 3 or 4 degrees is optimal

## Choosing polynomial degree using cross validation

glm.fit <- glm(wage ~ age, data = Wage)
coef(glm.fit)
cv.err <- cv.glm(Wage, glm.fit)
cv.err$delta
# Cross-validation estimate for the test error is approximately 1676.235

set.seed(3000)

cv.error <- rep(0, 5)

for (i in 1:5) {
 glm.fit <- glm(wage ~ poly(age, i), data = Wage)
 cv.error[i] <- cv.glm(Wage, glm.fit)$delta[1]
}

cv.error

min.d <- which.min(cv.error)

plot(1:5, cv.error, xlab = 'Degree', ylab = 'Test MSE', type = 'l')
points(min.d, cv.error[min.d], col = 'plum3', pch = 19)

# 3 looks like the optimal degree of d for the polynomial
# This result is similar to the ANOVA result which indicated either 3 or 4 would be the best fit. Cross validation also showed 3 or 4 would be best fit, with 4 being slightly better

age.range <- range(Wage$age)
age.grid <- seq(from = age.range[1], to = age.range[2])

fit <- lm(wage ~ poly(age, 3), data = Wage)
pred <- predict(fit, newdata = list(age = age.grid))

plot(wage ~ age, data = Wage, col = "darkgrey")
lines(age.grid, pred, col = "red", lwd = 2)

```


b) Fit a step function to predict wage using age, and perform cross validation to choose the optimal number of cuts. Make a plot of the fit obtained.

```{r}

options(scipen = 0)

table(cut(Wage$age, 4))

fit <- lm(wage ~ cut(age, 4), data = Wage)
coef(summary(fit))

# Need to make a plot


cv.error <- rep(NA,10)

for (i in 2:10) {
  Wage$age.cut <- cut(Wage$age, i)
  fit <- glm(wage ~ age.cut, data = Wage)
  cv.error[i] <- cv.glm(Wage, fit)$delta[1]
}

cv.error

min.d <- which.min(cv.error)

plot(2:10, cv.error[-1], xlab = 'Cuts', ylab = 'Test MSE', type = 'l')
points(min.d, cv.error[min.d], col = 'plum3', cex = 2, pch = 19)

# Cross validation indicates that 8 cuts will produce the smallest test MSE

# Prediction with 8 cuts 
fit <- glm(wage ~ cut(age, 8), data = Wage)
pred <- predict(fit, list(age = age.grid))

plot(wage ~ age, data = Wage, col = "grey")
lines(age.grid, pred, col = "red", lwd = 2)

```



# Exercise 7

Explore the relationships between some of these other predictors and wage, and use non-linear fitting techniques in order to fit flexible models to the data. Create plots of the results obtained, and write a summary of your findings.


```{r}

plot(Wage$jobclass, Wage$wage)
plot(Wage$race, Wage$wage)
plot(Wage$education, Wage$wage)


gam.m1 <- gam(wage ~ year + s(age, 4) + jobclass, data=Wage)
gam.m2 <- gam(wage ~ year + s(age, 4) + health, data=Wage)
gam.m3 <- gam(wage ~ year + s(age, 4) + jobclass + health, data=Wage)
gam.m4 <- gam(wage ~ year + s(age, 4) + jobclass + race, data=Wage)

options(scipen = 999)
anova(gam.m1, gam.m2, gam.m4, test="F")
anova(gam.m1, gam.m3, gam.m4, test="F")


par(mfrow=c(2,2))
plot(gam.m3, se=TRUE, col='blue')

# Job class and reported health are significant predictors of wage
# People in information jobs  are more likely to be receiving higher wages than people in industrial jobs. People who report "very good" health are more likely to be receiving higher wages than people who reported "good" health

# Wages are increasing as years pass. From age 20, wages increase until people hit age 40, where wages remain stagnant until they start decreasing at age 60 until age 80. 

```


# Exercise 9

a) Use the poly() function to ft a cubic polynomial regression to
predict nox using dis. Report the regression output, and plot
the resulting data and polynomial fits

```{r}

options(scipen = 0)

b1.fit <- lm(nox ~ poly(dis, 3), data = Boston)

coef(b1.fit)
summary(b1.fit)

dislims <- range(Boston$dis)
dis.grid <- seq(from=dislims[1], to=dislims[2], by=0.1)
pred <- predict(b1.fit, newdata = list(dis=dis.grid), se=TRUE)
se.bands <- cbind(pred$fit + 2 * pred$se.fit, pred$fit - 2 * pred$se.fit)


plot(Boston$dis, Boston$nox, xlim=dislims, cex=0.5, col="darkgrey")
lines(dis.grid, pred$fit, lwd=2, col="blue")
matlines(dis.grid, se.bands, lwd=1, col="blue", lty=3)

```

b) Plot the polynomial fts for a range of diferent polynomial degrees (say, from 1 to 10), and report the associated residual sum of squares

```{r}


b.errors <- rep(0, 10)

for(i in 1:10){
  fit <- lm(nox ~ poly(dis, i), data= Boston)
  b.errors[i] <- sum(fit$residuals^2)
}

b.errors

b.min <- which.min(b.errors)

plot(b.errors, type='b', xlab = 'Degrees', ylab='RSS')
points(b.min, b.errors[b.min], col = 'red', pch = 19)

# the RSS decreases with each added degree, resulting in the lowest RSS at 10 degrees
```


c) Perform cross-validation or another approach to select the optimal degree for the polynomial, and explain your results.

```{r}

set.seed(506)

b.errors <- rep(NA, 10)

for (i in 1:10){
  fit <- glm(nox ~ poly(dis, i), data=Boston)
  b.errors[i] <- cv.glm(Boston, fit, K=10)$delta[1]
}

b.errors

min.b <- which.min(b.errors)

plot(b.errors, type='b')
points(min.b, b.errors[min.b], col = 'red', pch = 19)

# Cross validation shows that 3 degrees has the lowest test MSE

```



d) Use the bs() function to ft a regression spline to predict nox using dis. Report the output for the fit using four degrees of freedom. How did you choose the knots? Plot the resulting ft.

```{r}

dislims <- range(Boston$dis)
dis.grid <- seq(from=dislims[1], to=dislims[2], by=0.1)
pred <- predict(b1.fit, newdata = list(dis=dis.grid), se=TRUE)
se.bands <- cbind(pred$fit + 2 * pred$se.fit, pred$fit - 2 * pred$se.fit)

fit <- lm(nox ~ bs(dis, knots = c(25, 40, 60)), data = Boston)
pred <- predict(fit, newdata = list(dis = dis.grid), se = T)


plot(Boston$dis, Boston$nox, col = "gray")
lines(dis.grid, pred$fit, lwd = 2)
lines(dis.grid, pred$fit + 2 * pred$se, lty = "dashed")
lines(dis.grid, pred$fit - 2 * pred$se, lty = "dashed")


dim(bs(Boston$dis, knots = c(25, 40, 60)))

dim(bs(Boston$dis, df = 6))

attr(bs(Boston$dis, df = 6), "knots")

# R chooses knots at 2.100175, 3.207450, 5.188425 (because this is the point at 25th, 50th, and 75th percentiles of the weighted distance data)

```


e) Now fit a regression spline for a range of degrees of freedom, and plot the resulting fits and report the resulting RSS. Describe the results obtained.

```{r}

r.error <- rep(NA, 20)

for (i in 1:20) {
 fit <- lm(nox ~ bs(dis, df=i), data = Boston)
  r.error[i] <- sum(fit$residuals^2)
}

r.error

r.min <- which.min(r.error)

plot(1:20, r.error[1:20], type = 'b')
points(r.min, r.error[r.min], col = 'red', pch = 19)

# The smallest RSS is when df = 19


```

f)

```{r, eval = F}

set.seed(506)
cv.error <- rep(NA, 20)

for (i in 1:20){
  fit <- glm(nox ~ bs(dis, df=i), data=Boston)
  cv.error[i] <- cv.glm(Boston, fit, K=10)$delta[1]
}

cv.error


min <- which.min(cv.error)



plot(2:20, cv.error[2:20], type='b')
points(min, cv.error[min], col = 'red', pch = 19)

# Cross validation selects df = 10 

```



# Exercise 10


a) Split the data into a training set and a test set. Using out-of-state tuition as the response and the other variables as the predictors, perform forward stepwise selection on the training set in order to identify a satisfactory model that uses just a subset of the predictors.


```{r}
?College

set.seed(777)

sample <- sample(1:nrow(College), nrow(College) / 2)
train <- College[- sample, ]
test <- College[sample, ]


col.fit <- regsubsets(Outstate ~ ., data = train, nvmax = 17, method = "forward")

col.summary <- summary(col.fit)

plot(col.summary$cp, xlab = "Number of Variables",
ylab = "Cp", type = "l")
which.min(col.summary$cp)
#14
points(14, col.summary$cp[10], col = "red", cex = 2,
pch = 20)

which.min(col.summary$bic)
#6

plot(col.summary$bic, xlab = "Number of Variables",
ylab = "BIC", type = "l")
points(6, col.summary$bic[6], col = "red", cex = 2,
pch = 20)

coef(col.fit,6)


plot(col.fit, scale = "r2")
plot(col.fit, scale = "adjr2")
plot(col.fit, scale = "Cp")
plot(col.fit, scale = "bic")

```


(b) Fit a GAM on the training data, using out-of-state tuition as the response and the features selected in the previous step as the predictors. Plot the results, and explain your findings.

```{r}

coef(col.fit,6)

gam_col <- gam(Outstate ~ Private + s(Room.Board, 3) + s(Terminal, 3) + s(perc.alumni, 3) + s(Expend, 3) + s(Grad.Rate, 3), data=train)

summary(gam_col)

par(mfrow=c(2,3))
plot(gam_col, se=TRUE, col="blue")


```

(c) Evaluate the model obtained on the test set, and explain the results obtained.

```{r}

set.seed(777)

pred <- predict(gam_col, newdata = test)
mean((test$Outstate-pred)^2)


# Test MSE is 4141933. 

```

(d) For which variables, if any, is there evidence of a non-linear relationship with the response?

```{r, eval = F}

summary(gam_col)

par(mfrow=c(2,3))
plot(College$Outstate, College$Room.Board)
plot(College$Outstate, College$Private)
plot(College$Outstate, College$Terminal)
plot(College$Outstate, College$perc.alumni)
plot(College$Outstate, College$Expend)
plot(College$Outstate, College$Grad.Rate)

## Nonlinear relationship between Outstate and Private, and Outstate and Expend 

```


...

