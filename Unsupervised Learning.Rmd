---
title: "Unsupervised Learning"
author: "Emily Blue"
date: "`2-26-24`"
output: openintro::lab_report
---
### Overview:

This R Markdown file includes my completion of **ISLR chapter 12**, exercises 8, 9, 12, and 13. 

#### Exercises 8: Calculate PVE
- **Objective**: Calculate PVE using the prcomp() function.

#### Exercises 9: Hierarchical Clustering
- **Objective**: Perform hierarchical clustering on the states.
- **Steps**: 
    - Cluster states using hierarchical clustering with complete linkage and Euclidean distance. 
    - Perform hierarchically cluster again with scaled variables. 

#### Exercises 12: Matrix Completion Function
- **Objective**: Write function to implement Iterative Algorithm for Matrix Completion. 
- **Steps**: 
    - Turn data frame into a matrix after scaling and centering columns. 
    - Compute principal component, compute the objective, and return estimated missing entries. 

#### Exercises 13: Healthy vs Diseased Genes
- **Objective**: Analyzing a gene expression dataset to differentiate between healthy and diseased groups.
- **Steps**: 
    - Apply hierarchical clustering to samples using distance and plot the dendrogram.
    - Perform Principal Component Analysis and K-Means Clustering to differentiate between groups. 
    
---

```{r load-packages, message=FALSE}
library(tidyverse)
library(openintro)
library(dplyr)
library(ISLR2)
data("USArrests")
data("College")
library(pls)
```


## Calculate PVE


On the USArrests data, calculate PVE. 

```{r code-chunk-label}

states <- row.names(USArrests)
names(USArrests)

pr.out <- prcomp(USArrests, scale = TRUE)
names(pr.out)

pr.out$center
pr.out$scale
pr.out$rotation

dim(pr.out$x)

pr.out$rotation = -pr.out$rotation
pr.out$x = -pr.out$x
biplot(pr.out, scale = 0)

pr.var <- pr.out$sdev^2
pr.var

pve <- pr.var / sum(pr.var)
pve

# First principal component explains 62.0% of variance in the data, the next principal component explains 24.7% of variance, the next explains 8.91% of variance in data, the next explains 4.33% of variance in data


par(mfrow = c(1, 2))
plot(pve, xlab = "Principal Component",
ylab = "Proportion of Variance Explained", ylim = c(0, 1),
type = "b")
plot(cumsum(pve), xlab = "Principal Component",
ylab = "Cumulative Proportion of Variance Explained",
ylim = c(0, 1), type = "b")

a <- c(1, 2, 8, -3)
cumsum(a)

```


## Hierarchical Clustering

Use the USArrests data to perform hierarchical clustering on the states.


### a) Using hierarchical clustering with complete linkage and Euclidean distance, cluster the states.

```{r}
set.seed(50)

x <- matrix(rnorm(50 * 2), ncol = 2)

hc.complete <- hclust(dist(USArrests), method = "complete")

par(mfrow = c(1, 3))
plot(hc.complete, main = "Complete Linkage",
xlab = "", sub = "", cex = .9)


```

### b) Cut the dendrogram at a height that results in three distinct clusters. Which states belong to which clusters?

```{r}

cutree(hc.complete, 3)

```

### c) Hierarchically cluster the states using complete linkage and Euclidean distance after scaling the variables. 

```{r}

scaleSD.Arrests = scale(USArrests)

hc.complete.scaleSD <- hclust(dist(scaleSD.Arrests), method = "complete")

plot(hc.complete.scaleSD, method = "complete")


```


### d) What efect does scaling the variables have on the hierarchical clustering obtained?

```{r}

cutree(hc.complete.scaleSD,3)

table(cutree(hc.complete,3),cutree(hc.complete.scaleSD,3))

```

After scaling the variables, the clusters obtained are similar to the original cluster with a few different classifications for states. Because the units are different due to differing state sizes, population, ect, scaling should be done to even out these differences. 


## Matrix Completion Function

### Write a function to implement Algorithm 12.1 using prcomp(). 

```{r}

# Centering and scaling each column to have mean 0 and variance 1. 
X <- data.matrix(scale(USArrests))
pcob <- prcomp(X)
summary(pcob)

# singular value decomposition
sX <- svd(X)
names(sX)
round(sX$v, 3)
pcob$rotation

t(sX$d * t(sX$u))
pcob$x


## Replace svd() with prcomp()
nomit <- 20

set.seed(50)
ina <- sample(seq(50), nomit)
inb <- sample(1:4, nomit, replace = TRUE)
Xna <- X
index.na <- cbind(ina, inb)
Xna[index.na] <- NA


# Step 1
fit.prcomp <- function(X, M = 1) {
 prob <- prcomp(X)
 with(prob,
u[, 1:M, drop = FALSE] %*%
(d[1:M] * t(v[, 1:M, drop = FALSE]))
)
}


fit.prcomp <- function(X, M = 1) {
 prob <- prcomp(X)
 with(prob,
sX$u[, 1:M, drop = FALSE] %*%
(sX$d[1:M] * t(sX$v[, 1:M, drop = FALSE]))
)
}

Xhat <- Xna
xbar <- colMeans(Xna, na.rm = TRUE)
Xhat[index.na] <- xbar[inb]

thresh <- 1e-7
rel_err <- 1
iter <- 0
ismiss <- is.na(Xna)
mssold <- mean((scale(Xna, xbar, FALSE)[!ismiss])^2)
mss0 <- mean(Xna[!ismiss]^2)

#Step 1
 while(rel_err > thresh) {
iter <- iter + 1
 # Step 2(a)
 Xapp <- fit.prcomp(Xhat, M = 1)
 # Step 2(b)
 Xhat[ismiss] <- Xapp[ismiss]
 # Step 2(c)
 mss <- mean(((Xna - Xapp)[!ismiss])^2)
 rel_err <- (mssold - mss) / mss0
 mssold <- mss
 cat("Iter:", iter, "MSS:", mss,
 "Rel. Err:", rel_err, "\n")
 }


# Correlation between the 20 imputed values and the actual values
cor(Xapp[ismiss], X[ismiss])

                                      
```


## Healthy vs Diseased Genes

There is a gene expression data set (Ch12Ex13.csv) that consists of 40 tissue samples with
measurements on 1,000 genes. The first 20 samples are from healthy patients, while the second 20 are from a diseased group.


### a) Load in the data
```{r, echo = F}

Samples <- read_csv("~/Downloads/Ch12Ex13.csv", 
     col_names = FALSE)
```

### b) Apply hierarchical clustering to the samples using correlation based distance, and plot the dendrogram. Do the genes separate the samples into the two groups? Do your results depend on the type of linkage used?

```{r}

set.seed(50)

# complete
hc.complete <- hclust(as.dist(1 - cor(Samples)), method='complete')

plot(hc.complete, main = "Complete Linkage",
xlab = "", sub = "", cex = .9)


# average
hc.average <- hclust(as.dist(1 - cor(Samples)), method = "average")

plot(hc.average, main = "Average Linkage",
xlab = "", sub = "", cex = .9)


#single
hc.single <- hclust(as.dist(1 - cor(Samples)), method = "single")

plot(hc.single, main = "Single Linkage",
xlab = "", sub = "", cex = .9)


par(mfrow = c(1, 3))
plot(hc.complete, main = "Complete Linkage",
xlab = "", sub = "", cex = .9)
plot(hc.average, main = "Average Linkage",
xlab = "", sub = "", cex = .9)
plot(hc.single, main = "Single Linkage",
xlab = "", sub = "", cex = .9)

```

The choice of method affects the results and clusters obtained. It shows two clusters formed for single and complete linkage while average linkage has three clusters. 

### c) Your collaborator wants to know which genes difer the most across the two groups. Suggest a way to answer this question, and apply it here.

I would suggest performing Principal Component Analysis (PCA) or K-Means Clustering to differentiate between the healthy and diseased groups


### Principal Component Analysis (PCA)

```{r}

Samples.labs <- Samples$labs
Samples.data <- Samples$data


pr.Samples <- prcomp(Samples, scale = TRUE)
summary(pr.Samples)
plot(pr.Samples)

dim(pr.Samples$x)
biplot(pr.Samples, scale = 0)

Cols <- function(vec) {
cols <- rainbow(length(unique(vec)))
return(cols[as.numeric(as.factor(vec))])
}


par(mfrow = c(1, 2))
plot(pr.Samples$x[, 1:2], col = Cols(Samples), pch = 19,
xlab = "Z1", ylab = "Z2")
plot(pr.Samples$x[, c(1, 3)], col = Cols(Samples), pch = 19,
xlab = "Z1", ylab = "Z3")


# PVE of each principal component and the cumulative PVE of each principal component
pve <- 100 * pr.out$sdev^2 / sum(pr.out$sdev^2)
par(mfrow = c(1, 2))
plot(pve, type = "o", ylab = "PVE",
xlab = "Principal Component", col = "blue")
plot(cumsum(pve), type = "o", ylab = "Cumulative PVE",
xlab = "Principal Component", col = "brown3")


```


### K-Means Clustering

```{r}

set.seed(40)

km.Samples <- kmeans(Samples, 2, nstart = 20)
summary(km.Samples)


km.Samples$cluster


par(mfrow = c(1, 2))
plot(x, col = (km.Samples$cluster + 1),
main = "K-Means Clustering Results with K = 2",
xlab = "", ylab = "", pch = 20, cex = 2)

```


...

