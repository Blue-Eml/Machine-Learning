---
title: "Tree-Based Methods"
author: "Emily Blue"
date: "`3-20-24`"
output: openintro::lab_report
---

### Overview:

This R Markdown file includes my completion of **ISLR chapter 8**, exercises 7, 9, and 10.

#### Exercises 7: Random Forests
- **Objective**: Best model to predict housing prices. 
- **Steps**:
    - Create a plot of variable importance. 
    - Create a plot of test error rates for a comprehensive range of trees (ntree) and variables (mtry).
    
#### Exercises 9: Decision Trees
- **Objective**: Predict customer purchases of Orange Juice brands.
- **Steps**:
  - Create a training set using random sampling and fit a decision tree to it.
  - Determine the tree size with the lowest cross-validated classification error rate.
  - Produce a pruned tree with optimal parameters using cross-validation.

#### Exercises 10: Boosting 
- **Objective**: Predict salary of Baseball players
- **Steps**: 
    - Create training set, perform boosting, and produce a plot of shrinkage values and its corresponding MSE. 
    - Determine the most important predictors in the boosted model.
    - Apply bagging to the training set and calculate the test MSE.

---


```{r load-packages, message=FALSE}
library(tidyverse)
library(openintro)
library(dplyr)
library(tree)
library(ISLR2)
library(gbm)
library(randomForest)
data("Boston")
?Boston

data("OJ")
data("Hitters")

```

## Random Forests

Using the Boston dataset, I explore random forest models predicting housing prices by testing a comprehensive number of trees (ntree) and number of variables (mtry) for the most accurate model. 

```{r ex7.1, echo = T}

# Setting random seed
set.seed(506)

# Creating training subset
train <- sample(1:nrow(Boston), nrow(Boston) / 2)
boston.test <- Boston[-train, "medv"]

# Boston Random Forest Model m = p, ntree = 500
rf.boston1 <- randomForest(medv ~ ., data = Boston,
  subset = train, mtry = ncol(Boston) - 1, ntree = 500, importance = TRUE)

rf.boston1
plot(rf.boston1)

yhat.rf1 <- predict(rf.boston1, newdata = Boston[-train, ])
rf1.MSE <- mean((yhat.rf1 - boston.test)^2)
rf1.MSE
# 9.351862

# plot of variable importance
varImpPlot(rf.boston1)

# Boston Random Forest Model with m = p/2, ntree = 500

set.seed(506)

train <- sample(1:nrow(Boston), nrow(Boston) / 2)
boston.test <- Boston[-train, "medv"]

rf.boston2 <- randomForest(medv ~ ., data = Boston,
  subset = train, mtry = ((ncol(Boston) - 1)/2), ntree = 500, importance = TRUE)

plot(rf.boston2)

yhat.rf2 <- predict(rf.boston2, newdata = Boston[-train, ])
rf2.MSE <- mean((yhat.rf2 - boston.test)^2)
rf2.MSE
# 9.349413


# Boston Random Forest Model with m = sqrt(p), ntree = 500

set.seed(506)

train <- sample(1:nrow(Boston), nrow(Boston) / 2)
boston.test <- Boston[-train, "medv"]

rf.boston3 <- randomForest(medv ~ ., data = Boston,
  subset = train, mtry = sqrt(ncol(Boston) - 1), ntree = 500, importance = TRUE)

plot(rf.boston3)

yhat.rf3 <- predict(rf.boston3, newdata = Boston[-train, ])
rf3.MSE <- mean((yhat.rf3 - boston.test)^2)
# 11.04574

plot(rf.boston1, rf1.MSE, type = "l") + 
  plot(rf.boston2, rf2.MSE, type = "l") +
  plot(rf.boston3, rf3.MSE, type = "l")


```


From this plot, the test MSE error is over 50 for one to five trees, but drops exponentially to a test error of 20 at around 10 trees. The test error continues to decease to about 10 at 80 trees, where it then steadies. A number of trees around 80 - 100 a lower test error. 

The plot illustrates the relationship between the number of trees in the random forest model and the corresponding test mean squared error (MSE).

Initially, the test error is high, exceeding 50 for the first few trees  (1 - 10. It then drops exponentially to an MSE of 20 at 10 trees. This reduction continues more gradually, stabilizing near a test error of 10 at approximately 80 to 100 trees. Beyond this point, the test error remains constant, indicating that the model's performance stabilizes, and additional trees do not significantly improve the predictive accuracy. 

The optimal test error is achieved with around 80 to 100 trees.


```{r}

rf_mse <- data.frame(mtry = integer(), 
                     ntree = integer(), 
                     mse = numeric())

mtry_vals = c(3,4,6,12)

ntree_vals = c(1:25, seq(30, 100, by = 5), seq(110, 500, by = 10))

set.seed(506)

for(mtry in mtry_vals) {
  for (ntree in ntree_vals){
    rf.boston <- randomForest(medv ~ ., data = Boston, 
                              subset = train, mtry = mtry, ntree = ntree)
    yhat.rf <- predict(rf.boston, newdata = Boston[- train, ])
    mse = mean((yhat.rf - boston.test)^2)
    rf_mse = rf_mse |> add_row(mtry = mtry, ntree = ntree, mse = mse)
  }
}

```


## Decision Trees

Using the OJ data set in the ISLR2 package. 

### (a) Create a training set containing a random sample of 800 observations, and a test set containing the remaining observations.

```{r 9.1, echo = F, message = FALSE}


set.seed(1070)

train <- sample(1:nrow(OJ), 800)
OJ.train <- OJ[train, ]
OJ.test <- sample(OJ[-train, ])

```


### (b) Fit a tree to the training data, with Purchase as the response and the other variables as predictors. Describe summary statistics. 

```{r 9.2, echo = T}

tree.OJ <- tree(Purchase ~ ., OJ,
                subset = train)

summary(tree.OJ)

# Error rate is 0.1538. 123/800 observations were misclassified
# Number of terminal modes in tree is 7

```


### (c) View tree summary and describe one of the terminal nodes. 

```{r 7.33, echo = T}

tree.OJ

#  2) LoyalCH < 0.482935 299  318.20 MM ( 0.22408 0.77592 )


```

In Terminal Node 2, LoyalCH (customer brand loyalty for Citrus Hill) was found to be the most significant predictor for this node and therefore splits were made based on LoyalCH. 

Splits were made mades on whether LoyalCH was less than  0.482935.

299 observations had LoyalCH < 0.482935.




### (d) Create a plot of the tree, and interpret the results.

```{r 7.4, echo = T}

plot(tree.OJ)
text(tree.OJ, pretty = 0)

```

The first node based on LoyalCH < 0.482935 is the most influential predictor in splitting groups. From this split, two categories were made. 

For group LoyalCH < 0.482935, the next node used LoyalCH < 0.276142 as the next condition to split. If observations met LoyalCH < 0.276142, they were placed in MM prediction. For observations with LoyalCH > 0.276142, SalePriceMM < 2.04 was used as a predictor for the next node. If SalePriceMM > 2.04, they were placed in CH. If condition SalePriceMM > 2.04 was met, SpecialCH < 0.5 was used as the predictor in the next node. Observations with SpecialCH < 0.5 were placed in MM prediction and SpecialCH > 0.5 were placed in CH prediction.

For group LoyalCH < 0.276142, the predictor used in the next node was LoyalCH < 0.753545. If this condition was not met (LoyalCH > 0.753545), they were placed in the CH prediction group. If the condition was met, PriceDiff < 0.015 was used as the predictor in the next node. If this condition was met, observations were placed in the MM prediction category. If the condition was not met, observations were placed in the CH category. 
Because the lines on the tree plot are longer, it shows that these conditions were able to more significant splits off these conditions. 

### (e) Apply the cv.tree() function to the training set in order to determine the optimal tree size.

```{r 9.5, echo=TRUE}

set.seed(1070)

tree.OJ <- tree(Purchase ~ ., OJ,
                subset = train)

Purchase.test <- OJ$Purchase[-train]

tree.pred <- predict(tree.OJ, OJ.test, type = "class")

table(tree.pred, Purchase.test)

(131 + 81) / 270
# 0.785

# The test error rate is 78.5%

```

### (f - k) Determine optimal tree size for the training set. Produce a plot of tree size and cross-validated classification error rate. 

```{r 9.645, echo = T}

cv.OJ <- cv.tree(tree.OJ, FUN = prune.misclass)
names(cv.OJ)

cv.OJ
# tree with 7 terminal nodes results in 139 cross-validation errors

par(mfrow = c(1, 2))

# tree size vs classification error rate
plot(cv.OJ$size, cv.OJ$dev, type = "b")

# tree with 7 nodes results in lowest classification error rate

prune.OJ <- prune.misclass(tree.OJ, best = 7)
plot(prune.OJ)
text(prune.OJ, pretty = 0)

# Training error rate

summary(prune.OJ)

# Training error rate is 0.1538
# Training error for pruned tree is same as unpruned tree

prune.tree.pred <- predict(prune.OJ, OJ.test, type = "class")
table(prune.tree.pred, Purchase.test)

(131 + 81) / 270
# 0.785

# Test error rate is the same for both pruned and unpruned trees

```

The unpruned tree had 7 nodes and cross validation showed the optimal tree size is 7. 

(Pruned tree again with 5 nodes)
```{r 9.61, echo = T}

prune.OJ.5 <- prune.misclass(tree.OJ, best = 5)
plot(prune.OJ.5)
text(prune.OJ.5, pretty = 0)

# Training error rate

summary(prune.OJ.5)

# Training error rate is 0.1538

prune.tree.pred.5 <- predict(prune.OJ.5, OJ.test, type = "class")
table(prune.tree.pred.5, Purchase.test)

(131 + 81) / 270
# 0.785

```

Again, test error rate and training error rate was the same when using 5 nodes. 

## Boosting 

Use boosting to predict Salary in the Hitters data set.


### (a) Remove the observations for unknown salary information then log-transform the salaries.

```{r 10.1, echo  = T}

library(leaps)
?Hitters

head(Hitters)

sum(is.na(Hitters$Salary))
Hitters <- na.omit(Hitters)

dim(Hitters)

sum(is.na(Hitters))

Hitters$LogSalary <- log(Hitters$Salary)


```


### (b) Create a training set consisting of the first 200 observations, and  a test set consisting of the remaining observations

```{r 10.2, echo  = T}

train <- sample(1:nrow(Hitters), 200)

hit.train <- Hitters[train, ]
hit.test <- Hitters[-train, ]

```

### (c - d) Perform boosting on the training set with 1,000 trees for a range of values of the shrinkage parameter Î». Produce a plot with different shrinkage and the corresponding training set MSE.
```{r idk anymore, echo  = T}

values <- seq(0.001, 0.05, by = 0.0025)

train_mse <- length(values)

for (i in 1:length(values)) {
  boost.hit <- gbm(LogSalary ~ . - Salary, data = hit.train, distribution = "gaussian", n.trees = 1000, shrinkage = values[i])
  
  yhat.boost <- predict(boost.hit, newdata = hit.test, n.trees = 1000)
  
  train_mse[i] <- mean((yhat.boost - hit.test$LogSalary)^2)
}

plot(values, train_mse, xlim = c(0, 0.2), xlab = "Shrinkage  Values", ylab = "MSE")

which.min(train_mse)
#1

```


```{r idk, echo  = T}

set.seed(1070)
values <- seq(0.001, 0.05, by = 0.0025)

test_mse <- length(values)

for (i in 1:length(values)) {
  boost.hit <- gbm(LogSalary ~ ., data = hit.test, distribution = "gaussian", n.trees = 1000, shrinkage = values[i])
  
  yhat.boost <- predict(boost.hit, newdata = hit.test, n.trees = 1000)
  
  test_mse[i] <- mean((yhat.boost - hit.test$LogSalary)^2)
}

plot(values, test_mse, xlab = "MSE", ylab = "Shrinkage Values")

which.min(test_mse)
#20

```

### (e) Compare the test MSE of boosting to the test MSE that results from other regression approaches. 

```{r 10.e, Echo = T}


lm.hit <- lm(LogSalary ~ . - Salary, data = hit.train)

lm.pred <- predict(lm.hit, hit.test)

lm_mse <- mean((lm.pred - hit.test$LogSalary)^2)
lm_mse

# MSE is 0.4737885

```

```{r 10.e33, Echo = T}

library(glmnet)

x <- model.matrix(LogSalary ~ . - Salary, Hitters)[, -1]
y <- Hitters$Salary

grid <- 10^seq(10, -2, length = 100)
ridge.mod <- glmnet(x, y, alpha = 0, lambda = grid)

predict(ridge.mod, s = 50, type = "coefficients")[1:20, ]

set.seed(1070)
train <- sample(1:nrow(x), nrow(x) / 2)
test <- (-train)
y.test <- y[test]


ridge.mod <- glmnet(x[train, ], y[train], alpha = 0,
lambda = grid, thresh = 1e-12)
ridge.pred <- predict(ridge.mod, s = 4, newx = x[test, ])
mean((ridge.pred - y.test)^2)

mean((mean(y[train]) - y.test)^2)
# MSE is 179965.1
```

The models performing linear regression and ridge regression have higher MSE's than the model performing boosting. 

### (f) Which variables appear to be the most important predictors in the boosted model?
```{r 10.f, echo = T}

boost.hit <- gbm(Salary ~ . - LogSalary, data = hit.train, distribution = "gaussian", n.trees = 500, shrinkage = values[which.min(test_mse)])

summary(boost.hit)
```

The most important predictors are CRBI (number of runs battled in career) and CWalks (number of walks during career). 

### (g) Now apply bagging to the training set. What is the test set MSE? 

```{r bagging , echo = T}
set.seed(1070)

bag.hit <- randomForest(Salary ~ ., data = hit.train, mtry = 19, importance = T)

bag.hit

yhat.bag <- predict(bag.hit, newdata = hit.test)

mean((yhat.bag - hit.test$Salary)^2)
# test set MSE is 7052.907

```
...

